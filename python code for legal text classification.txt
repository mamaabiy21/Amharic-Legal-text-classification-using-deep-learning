******Mounted at /content/drive********
from google.colab import drive
drive.mount('/content/drive')
*********data preprocessing***********
import pandas as pd

# Load your CSV file
df = pd.read_csv("/content/drive/MyDrive/merged_dataset.csv")  # Replace with your actual file path

# View the first few rows to find the correct class column name
print(df.head())
print(df.columns)

# Count the number of entries for each class
class_counts = df['Class'].value_counts()  # Replace 'Class' if the column name is different
print(class_counts)
data.head(8025)

******CNN model***********

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import seaborn as sns # Import seaborn
# Import necessary metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# 1. Load CSV
df = pd.read_csv("/content/drive/MyDrive/merged_dataset.csv")

# 2. Preprocess
texts = df['Sentence'].astype(str).tolist()
labels = df['Class'].astype(str).tolist()

# Encode labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(labels)

# Tokenize text
vocab_size = 20000
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)

# Pad sequences
maxlen = 100
X = pad_sequences(X, padding='post', maxlen=maxlen)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. CNN Model
embedding_dim = 150
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),
    Conv1D(128, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(set(y)), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 4. Train model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# 5. Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Predict on the test data
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Get the class with highest probability


# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Confusion matrix
plt.figure(figsize=(7, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Greens', xticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'], yticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'])
plt.title("Confusion Matrix")
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# === 8. Plot Accuracy & Loss ===
plt.figure(figsize=(14, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='x')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss', marker='x')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
***********CNN + BiGRU model***********
import pandas as pd
import numpy as np
import re
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Embedding, Conv1D, MaxPooling1D,
                                     Bidirectional, GRU, Dense, Dropout, Layer, GlobalMaxPooling1D) # Import GlobalMaxPooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module and assign it to the plt alias
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import EarlyStopping
import seaborn as sns
import fasttext # Import the fasttext library

# ... (rest of your code)


# ... (rest of your code remains the same)

# ... (rest of your code remains the same)
# -----------------------
# 1. Load CSV
df = pd.read_csv("/content/drive/MyDrive/merged_dataset.csv")

# 2. Preprocess
texts = df['Sentence'].astype(str).tolist()
labels = df['Class'].astype(str).tolist()
# -----------------------
# -----------------------
# Step 2: Preprocess Labels
# -----------------------
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(labels)

# -----------------------
# Step 3: Tokenize and Pad Texts
# -----------------------
vocab_size = 20000
maxlen = 100

tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, padding='post', maxlen=maxlen)

# -----------------------
# Step 4: Train-test Split
# -----------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -----------------------
# Step 5: Build CNN + BiGRU Model
# -----------------------
embedding_dim = 300

input_layer = Input(shape=(maxlen,))
embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen)(input_layer)

# CNN layer
conv_layer = Conv1D(128, 5, activation='relu', padding='same')(embedding_layer)

# BiGRU layer
bigru_layer = Bidirectional(GRU(64, return_sequences=True))(conv_layer)

# Global Max Pooling
pooling_layer = GlobalMaxPooling1D()(bigru_layer)

# Fully Connected Layers
dropout1 = Dropout(0.5)(pooling_layer)
dense1 = Dense(64, activation='relu')(dropout1)
dropout2 = Dropout(0.5)(dense1)
output_layer = Dense(len(np.unique(y)), activation='softmax')(dropout2)

# Compile Model
model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# -----------------------
# Step 6: Train the Model
# -----------------------
model.summary()

history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1)

# -----------------------
# Step 7: Evaluate
# Step 8: Evaluate
# -----------------------

# Predict on the test data # This should be before accuracy and other metrics calculation
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Get the class with highest probability

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Confusion matrix
plt.figure(figsize=(7, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Greens', xticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'], yticklabels=['Civil law', 'Human trafficking law','Constitutional law','Ant-corruption law', 'Criminal law', 'Family law', 'Labor law', 'commercial law'])
plt.title("Confusion Matrix")
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# === 8. Plot Accuracy & Loss ===
plt.figure(figsize=(14, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='*')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss', marker='*')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

***********CNN + BiLSTM model***********
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, Bidirectional, GRU, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.layers import Input, Embedding, Conv1D, Bidirectional, GRU, GlobalMaxPooling1D, Dense, Dropout, LSTM # Import LSTM
import tensorflow as tf
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module and assign it to the plt alias
import seaborn as sns # Import seaborn This line was missing

# -----------------------
# Step 1: Load Dataset
# -----------------------
df = pd.read_csv("/content/drive/MyDrive/merged_dataset.csv")

# 2. Preprocess
texts = df['Sentence'].astype(str).tolist()
labels = df['Class'].astype(str).tolist()
# -----------------------
# Step 2: Preprocess Labels
# -----------------------
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(labels)

# -----------------------
# Step 3: Tokenize and Pad
# -----------------------
vocab_size = 20000
maxlen = 100

tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, padding='post', maxlen=maxlen)

# -----------------------
# Step 4: Train-test Split
# -----------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -----------------------
# Step 5: Build CNN + BiLSTM Model
# -----------------------
embedding_dim = 300

input_layer = Input(shape=(maxlen,))
embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen)(input_layer)

# CNN layer
conv_layer = Conv1D(128, 5, activation='relu', padding='same')(embedding_layer)

# BiLSTM layer
bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))(conv_layer)

# Global Max Pooling
pooling_layer = GlobalMaxPooling1D()(bilstm_layer) # Change MaxPooling1D to GlobalMaxPooling1D


# Fully connected layers
dropout1 = Dropout(0.5)(pooling_layer)
dense1 = Dense(64, activation='relu')(dropout1)
dropout2 = Dropout(0.5)(dense1)
output_layer = Dense(len(np.unique(y)), activation='softmax')(dropout2)


# Create Model
model = Model(inputs=input_layer, outputs=output_layer)

# Compile
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# -----------------------
# Step 6: Train the Model
# -----------------------
model.summary()

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# -----------------------
# Step 7: Evaluate
# -----------------------
# Predict on the test data # This should be before accuracy and other metrics calculation
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Get the class with highest probability

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Confusion matrix
plt.figure(figsize=(7, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Greens', xticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'], yticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'])

plt.title("Confusion Matrix")
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# === 8. Plot Accuracy & Loss ===
plt.figure(figsize=(14, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='*')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss', marker='*')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

************CNN+Self-Attention model**********
import numpy as np
import pandas as pd
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.layers import Layer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import gensim #fixed: import the gensim library to load word2vec format
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module and assign it to the plt alias
import seaborn as sns
import re
# 1. Load your Amharic dataset (assume columns: 'text', 'label')
df = pd.read_csv("/content/drive/MyDrive/merged_dataset.csv")
def clean_token(token):
    # Example: remove punctuation and special characters
    token = re.sub(r'[^\w\s]', '', token)
    return token.strip()
# 2. Preprocess
texts = df['Sentence'].astype(str).tolist()
labels = df['Class'].astype(str).tolist()


# Get tokens from 'texts' # This line was changed
tokens = [t for text in texts for t in text.split()]
tokens = [clean_token(t) for t in tokens]  # Now, clean the tokens


# 2. Tokenize
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1
max_seq_len = 200
X = pad_sequences(sequences, maxlen=max_seq_len)

# 3. Encode labels
le = LabelEncoder()
y = le.fit_transform(labels)
num_classes = len(le.classes_)

# 4. Load Amharic FastText embeddings (cc.am.300.bin)
# Use gensim to load the FastText embeddings, specifying the encoding as 'utf-8'
embedding_model = gensim.models.FastText.load_fasttext_format('/content/drive/MyDrive/cc.am.300.bin', encoding='utf-8') # Assuming it's in FastText format

embedding_dim = 300
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in word_index.items():
    if word in embedding_model.wv:  # Check if word is in model's vocabulary
        embedding_matrix[i] = embedding_model.wv[word]


# 5. Self-Attention Layer
class SelfAttention(Layer):
    def __init__(self, **kwargs):
        super(SelfAttention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], input_shape[-1]),
                                 initializer='random_normal', trainable=True)
        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1],),
                                 initializer='zeros', trainable=True)
        self.u = self.add_weight(name='att_score', shape=(input_shape[-1], 1),
                                 initializer='random_normal', trainable=True)
        super(SelfAttention, self).build(input_shape)

    def call(self, x):
        u_it = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)
        scores = tf.nn.softmax(tf.tensordot(u_it, self.u, axes=1), axis=1)
        output = tf.reduce_sum(x * scores, axis=1)
        return output

# 6. Build the model
input_layer = Input(shape=(max_seq_len,))
embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim,
                            weights=[embedding_matrix], input_length=max_seq_len,
                            trainable=True)(input_layer)
learning_rate = 0.00001  # You can try 1e-3, 1e-4, etc.
optimizer = Adam(learning_rate=learning_rate)
cnn = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding_layer)
pool = GlobalMaxPooling1D()(cnn)
att = SelfAttention()(cnn)  # If attention is simple
concat = tf.keras.layers.concatenate([pool, att])
drop = Dropout(0.5)(concat)
dense = Dense(32, activation='relu')(drop)
output = Dense(num_classes, activation='softmax')(dense)

model = Model(inputs=input_layer, outputs=output)
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

# 7. Train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42) # Combine train_test_split
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #fixed: X_test, y_test split was missing
early_stop = EarlyStopping(patience=3, restore_best_weights=True)

history = model.fit(X_train, y_train,
          validation_data=(X_test, y_test), # Use X_test, y_test for validation
          epochs=16,
          batch_size=32,
          callbacks=[early_stop])
#fixed: evaluate the model on X_test and y_test
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Remove extra train_test_split

# Predict on the test data # This should be before accuracy and other metrics calculation
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Get the class with highest probability

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_)) # Replace label_encoder with le

# Confusion matrix
plt.figure(figsize=(7, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Greens', xticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'], yticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'])

plt.title("Confusion Matrix")
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# === 8. Plot Accuracy & Loss ===
plt.figure(figsize=(14, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='*') # Use val_accuracy for validation accuracy
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss', marker='*') # Use val_loss for validation loss
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

***********CNN + BiGRU + self Attention module***********

import pandas as pd
import numpy as np
import re
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Embedding, Conv1D, MaxPooling1D,
                                     Bidirectional, GRU, Dense, Dropout, Layer, GlobalAveragePooling1D) # Import GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module and assign it to the plt alias
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import EarlyStopping
import seaborn as sns
import fasttext # Import the fasttext library

# ========== Step 1: Load and Clean Data ==========
# 1. Load your Amharic dataset (assume columns: 'text', 'label')
df = pd.read_csv("/content/drive/MyDrive/merged_dataset.csv")

# 2. Preprocess
texts = df['Sentence'].astype(str).tolist()
labels = df['Class'].astype(str).tolist()

def clean_text(text):
    text = re.sub(r'[^\u1200-\u137F\s]', '', text)  # Keep Amharic characters
    return text.strip()

texts = [clean_text(t) for t in texts]

# ========== Step 2: Tokenization and Padding ==========
max_vocab = 10000
max_len = 100

tokenizer = Tokenizer(num_words=max_vocab)
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, maxlen=max_len)

# Encode labels
le = LabelEncoder()
y = le.fit_transform(labels) # Change to single-label encoding
# Get the number of unique classes
num_classes = len(le.classes_) #fixed: Define num_classes

# ========== Step 3: Load FastText Embeddings ==========
embedding_index = {}
# Try opening the file with 'latin-1' encoding
model_path = '/content/drive/MyDrive/cc.am.300.bin'
ft_model = fasttext.load_model(model_path)

embedding_dim = 300
word_index = tokenizer.word_index
embedding_matrix = np.zeros((max_vocab, embedding_dim))

for word, i in word_index.items():
    if i < max_vocab:
        try:
            embedding_vector = ft_model.get_word_vector(word)  # Get vector for the word
            embedding_matrix[i] = embedding_vector  # Assign the embedding vector
        except KeyError:
            pass

# ========== Step 4: Attention Layer ==========
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight',
                                 shape=(input_shape[-1], 1),
                                 initializer='random_normal',
                                 trainable=True)
        self.b = self.add_weight(name='att_bias',
                                 shape=(input_shape[1], 1),
                                 initializer='zeros',
                                 trainable=True)
        super().build(input_shape)

    def call(self, x):
        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)
        alpha = tf.nn.softmax(e, axis=1)
        output = tf.reduce_sum(x * alpha, axis=1)
        return output

# ========== Step 5: Build Model ==========
# ========== Step 5: Build Model ==========
input_layer = Input(shape=(max_len,))

# Assuming you are using the pre-trained embeddings
vocab_size = embedding_matrix.shape[0] #fixed: Define vocab_size based on embedding matrix
embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim,  # Use vocab_size and embedding_dim here
                            weights=[embedding_matrix],
                            input_length=max_len,
                            trainable=True)(input_layer)

embedding = Embedding(input_dim=vocab_size, output_dim=128)(input_layer) #fixed: Use vocab_size here as well
bigru = Bidirectional(GRU(64))(embedding)  # Now receives 3D input: (None, max_len, 128)
dense = Dense(32, activation='relu')(bigru)
output = Dense(num_classes, activation='softmax')(dense) #fixed: num_classes is now defined

model = Model(inputs=input_layer, outputs=output)
#lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1)
conv = Conv1D(128, kernel_size=3, activation='relu')(embedding_layer)
#pool = GlobalMaxPooling1D(pool_size=2)(conv) # Remove pool_size argument
pool = MaxPooling1D()(conv)  # Use GlobalMaxPooling1D without pool_size
#pool = GlobalMaxPooling1D()(conv) #changed from GlobalMaxPooling1D to GlobalAveragePooling1D to ensure 3D output
bigru = Bidirectional(GRU(64, return_sequences=True))(pool) #now pool provides a 3D output [samples, timesteps, features] to bigru
attention = AttentionLayer()(bigru)
drop = Dropout(0.3)(attention)
output = Dense(len(le.classes_), activation='softmax')(drop) # Change output layer units

model = Model(inputs=input_layer, outputs=output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Change loss function

model.summary()

# ========== Step 6: Train Model ==========
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42) # Perform train-test split here

# Train the model and store the training history
history = model.fit(X_train, y_train,
          validation_data=(X_test, y_test), # Use X_test, y_test for validation
          epochs=12,
          batch_size=32,
          verbose=2)

#fixed: evaluate the model on X_test and y_test
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Removed this line as it's already done above
loss, accuracy = model.evaluate(X_test, y_test)
# Predict on the test data # This should be before accuracy and other metrics calculation
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Get the class with highest probability

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_)) # Replace label_encoder with le

# Confusion matrix
plt.figure(figsize=(7, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Greens', xticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'], yticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'])

plt.title("Confusion Matrix")
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# === 8. Plot Accuracy & Loss ===
plt.figure(figsize=(14, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='*') # Use val_accuracy for validation accuracy
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss', marker='*') # Use val_loss for validation loss
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

**********CNN + BiLSTM + Self-Attention***********

import pandas as pd
import numpy as np
import re
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Embedding, Conv1D, MaxPooling1D,
                                     Bidirectional, LSTM, Dense, Dropout, Layer)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
#from tensorflow.keras.utils import to_categorical #removed: No need for to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import fasttext # Import the fasttext library
import io # Import the io library
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module and assign it to the plt alias
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import EarlyStopping
import seaborn as sns

# ========== Step 1: Load and Clean Data ==========
df = pd.read_csv("/content/drive/MyDrive/merged_dataset.csv")

# 2. Preprocess
texts = df['Sentence'].astype(str).tolist()
labels = df['Class'].astype(str).tolist()

def clean_text(text):
    # Only keep Amharic characters and spaces
    text = re.sub(r'[^\u1200-\u137F\s]', '', text)
    return text.strip()

texts = [clean_text(t) for t in texts]

# ========== Step 2: Tokenize and Pad ==========
max_vocab = 10000
max_len = 100

tokenizer = Tokenizer(num_words=max_vocab)
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, maxlen=max_len)

# Encode labels
le = LabelEncoder()
y = le.fit_transform(labels) #changed: Use LabelEncoder for single-label encoding

# ========== Step 3: Load FastText Amharic Embeddings ==========
# Load the FastText model using fasttext.load_model
model_path = '/content/drive/MyDrive/cc.am.300.bin'
ft_model = fasttext.load_model(model_path)

embedding_dim = 300
word_index = tokenizer.word_index
embedding_matrix = np.zeros((max_vocab, embedding_dim))

for word, i in word_index.items():
    if i < max_vocab:
        try:
            embedding_vector = ft_model.get_word_vector(word)  # Get vector using get_word_vector
            embedding_matrix[i] = embedding_vector
        except KeyError:
            pass  # Handle words not found in the model


# ========== Step 4: Define Self-Attention Layer ==========
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight',
                                 shape=(input_shape[-1], 1),
                                 initializer='random_normal',
                                 trainable=True)
        self.b = self.add_weight(name='att_bias',
                                 shape=(input_shape[1], 1),
                                 initializer='zeros',
                                 trainable=True)
        super().build(input_shape)

    def call(self, x):
        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)
        alpha = tf.nn.softmax(e, axis=1)
        output = tf.reduce_sum(x * alpha, axis=1)
        return output

# ========== Step 5: Build CNN + BiLSTM + Attention Model ==========
input_layer = Input(shape=(max_len,))
embedding_layer = Embedding(input_dim=max_vocab,
                            output_dim=embedding_dim,
                            weights=[embedding_matrix],
                            input_length=max_len,
                            trainable=True)(input_layer)

conv = Conv1D(128, kernel_size=5, activation='relu')(embedding_layer)
pool = MaxPooling1D(pool_size=2)(conv)
bilstm = Bidirectional(LSTM(64, return_sequences=True))(pool)
attention = AttentionLayer()(bilstm)
drop = Dropout(0.5)(attention)
output = Dense(len(le.classes_), activation='softmax')(drop) #changed: Output layer units to match number of classes

model = Model(inputs=input_layer, outputs=output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) #changed: Loss function for single-label

model.summary()

# ========== Step 6: Train the Model ==========
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

history = model.fit(X_train, y_train,
          validation_data=(X_val, y_val),
          epochs=12,
          batch_size=32,
          verbose=2)
#removed: Extra train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #This should be here.
loss, accuracy = model.evaluate(X_test, y_test)
# Predict on the test data # This should be before accuracy and other metrics calculation
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Get the class with highest probability

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_)) # Replace label_encoder with le

# Confusion matrix
plt.figure(figsize=(7, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Greens', xticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'], yticklabels=['Civil law', 'Human trafficking law','Constitutional law', 'Ant-corruption law','Criminal law', 'Family law', 'Labor law', 'commercial law'])

plt.title("Confusion Matrix")
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# === 8. Plot Accuracy & Loss ===
plt.figure(figsize=(14, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='*') # Use val_accuracy for validation accuracy
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss', marker='*') # Use val_loss for validation loss
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()